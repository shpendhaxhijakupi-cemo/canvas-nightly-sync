name: Nightly Canvas Export (Cohorts)

on:
  workflow_dispatch:
  schedule:
    - cron: "0 5 * * *"

permissions:
  contents: read

jobs:
  run-export:
    runs-on: ubuntu-24.04
    strategy:
      fail-fast: false
      matrix:
        include:
          # B2C split (no per-chunk soft-delete)
          - key: B2C_P1
            table: "B2C Courses Enrollment"
            soft_delete: "false"
          - key: B2C_P2
            table: "B2C Courses Enrollment"
            soft_delete: "false"

          # Other cohorts (with soft-delete)
          - key: SOUTHLANDS
            table: "Southlands Courses Enrollment"
            soft_delete: "true"
          - key: PHOENIX
            table: "Phoenix Christian Courses Enrollment"
            soft_delete: "true"
          - key: ONE_TWO_ONE_ONE
            table: "1211 Courses Enrollment"
            soft_delete: "true"
          - key: ROCKCHRISTIAN
            table: "Rock Christian Courses Enrollment"
            soft_delete: "true"
          - key: GANEVAS
            table: "Ganevas Courses Enrollment"
            soft_delete: "true"
          - key: NHCC
            table: "NHCC Courses Enrollment"
            soft_delete: "true"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests

      - name: Resolve Canvas IDs for cohort
        id: resolve_ids
        shell: bash
        env:
          INCLUDE_CANVAS_IDS_P1: ${{ secrets.INCLUDE_CANVAS_IDS_P1 }}
          INCLUDE_CANVAS_IDS_P2: ${{ secrets.INCLUDE_CANVAS_IDS_P2 }}
          IDS_SOUTHLANDS:        ${{ secrets.IDS_SOUTHLANDS }}
          IDS_PHOENIX:           ${{ secrets.IDS_PHOENIX }}
          IDS_1211:              ${{ secrets.IDS_1211 }}
          IDS_ROCKCHRISTIAN:     ${{ secrets.IDS_ROCKCHRISTIAN }}
          IDS_GANEVAS:           ${{ secrets.IDS_GANEVAS }}
          IDS_NHCC:              ${{ secrets.IDS_NHCC }}
        run: |
          case "${{ matrix.key }}" in
            B2C_P1)          echo "ids=$INCLUDE_CANVAS_IDS_P1" >> $GITHUB_OUTPUT ;;
            B2C_P2)          echo "ids=$INCLUDE_CANVAS_IDS_P2" >> $GITHUB_OUTPUT ;;
            SOUTHLANDS)      echo "ids=$IDS_SOUTHLANDS"        >> $GITHUB_OUTPUT ;;
            PHOENIX)         echo "ids=$IDS_PHOENIX"           >> $GITHUB_OUTPUT ;;
            ONE_TWO_ONE_ONE) echo "ids=$IDS_1211"              >> $GITHUB_OUTPUT ;;
            ROCKCHRISTIAN)   echo "ids=$IDS_ROCKCHRISTIAN"     >> $GITHUB_OUTPUT ;;
            GANEVAS)         echo "ids=$IDS_GANEVAS"           >> $GITHUB_OUTPUT ;;
            NHCC)            echo "ids=$IDS_NHCC"              >> $GITHUB_OUTPUT ;;
            *) echo "Unknown cohort key: ${{ matrix.key }}"; exit 1 ;;
          esac

      - name: Build per-course CSV for Airtable (${{ matrix.table }})
        env:
          API_URL: ${{ secrets.CANVAS_API_URL }}
          API_KEY: ${{ secrets.CANVAS_API_KEY }}
          IDS:     ${{ steps.resolve_ids.outputs.ids }}
        run: |
          python - <<'PY'
          import csv, os, time, sys
          from datetime import datetime
          import requests

          API_URL = os.environ["API_URL"].rstrip("/")
          API_KEY = os.environ["API_KEY"]
          IDS     = [s.strip() for s in (os.environ.get("IDS") or "").split(",") if s.strip()]
          HEADERS = {"Authorization": f"Bearer {API_KEY}"}
          PER_PAGE = 100
          START_MONTH = 8

          def iso(dt): return datetime.fromisoformat(dt.replace("Z","+00:00")) if dt else None
          def school_year(dt):
              if not dt: return ""
              y = dt.year
              return f"{y-1}-{y}" if dt.month < START_MONTH else f"{y}-{y+1}"

          def paged(url, params=None):
              params = dict(params or {})
              while True:
                  r = requests.get(url, headers=HEADERS, params=params, timeout=60)
                  if r.status_code in (429,500,502,503,504):
                      time.sleep(1); continue
                  r.raise_for_status()
                  data = r.json()
                  if isinstance(data, list):
                      for x in data: yield x
                  else:
                      yield data
                  link = r.headers.get("Link","")
                  nxt = None
                  for part in link.split(","):
                      if 'rel="next"' in part:
                          nxt = part[part.find("<")+1: part.find(">")]
                          break
                  if not nxt: break
                  url, params = nxt, None

          out = "Canvas_Enrollment_Courses.csv"
          with open(out, "w", newline="", encoding="utf-8") as f:
              w = csv.writer(f)
              w.writerow([
                  "Enrollment Course Key","Enrollment Key","Student Canvas ID","Student Name",
                  "School Year","Course ID","Course Code","Course Name",
                  "Course Start Date","Course End Date","Assigned Teachers",
                  "Current Score","Final Score","Passed?",
                  "Total Assignments","Completed Assignments","Unsubmitted Assignments","% Completed"
              ])

              if not IDS:
                  print("[fatal] No Canvas IDs provided for this cohort.", file=sys.stderr)
                  sys.exit(1)

              bad_patterns = ("NPS","feedback","survey")

              for uid in IDS:
                  try:
                      p = requests.get(f"{API_URL}/users/{uid}/profile", headers=HEADERS, timeout=30)
                      name = (p.json().get("name") or f"User {uid}") if p.ok else f"User {uid}"

                      for enr in paged(f"{API_URL}/users/{uid}/enrollments",
                                       {"type[]":"StudentEnrollment","include[]":"grades","per_page":PER_PAGE}):
                          cid = enr.get("course_id")
                          if not cid: continue
                          grades = enr.get("grades") or {}
                          cur = grades.get("current_score")
                          fin = grades.get("final_score")

                          c = requests.get(f"{API_URL}/courses/{cid}", headers=HEADERS, timeout=60)
                          course = c.json() if c.ok else {}
                          cstart = iso(course.get("start_at")) or iso(enr.get("start_at"))
                          cend   = iso(course.get("end_at")) or iso(enr.get("end_at"))
                          sy     = school_year(cstart or iso(enr.get("created_at"))) or "unknown"
                          if not (str(uid).strip() and str(cid).strip() and sy != "unknown"):
                              continue

                          tnames = []
                          for t in paged(f"{API_URL}/courses/{cid}/enrollments",
                                         {"type[]":"TeacherEnrollment","per_page":PER_PAGE}):
                              u = t.get("user") or {}
                              nm = u.get("name") or u.get("sortable_name")
                              if nm: tnames.append(nm)
                          teachers = ", ".join(sorted(set(tnames)))

                          passed = ""
                          try:
                              if fin is not None: passed = "Yes" if float(fin) >= 60.0 else "No"
                          except Exception: passed = ""

                          total = comp = unsubmitted = 0
                          for a in paged(f"{API_URL}/courses/{cid}/assignments"):
                              nm = (a.get("name") or "")
                              if not a.get("published"): continue
                              if nm == "End of Unit Feedback (Mandatory)": continue
                              if any(x.lower() in nm.lower() for x in bad_patterns): continue
                              total += 1
                              s = requests.get(
                                f"{API_URL}/courses/{cid}/assignments/{a['id']}/submissions/{uid}",
                                headers=HEADERS, timeout=30)
                              if s.ok:
                                  sub = s.json()
                                  state = sub.get("workflow_state") or "unsubmitted"
                                  if sub.get("excused"):
                                      comp += 1
                                  elif state in ("graded","submitted") and (sub.get("grade") not in (None,"-","")):
                                      comp += 1
                                  else:
                                      unsubmitted += 1
                              else:
                                  unsubmitted += 1
                          pct = f"{(comp/total*100):.2f}%" if total else "0.00%"

                          enr_key = f"{uid}-{sy}"
                          key     = f"{uid}-{sy}-{cid}"

                          w.writerow([
                              key, enr_key, uid, name,
                              sy, cid, course.get("course_code") or "", course.get("name") or "",
                              cstart.isoformat() if cstart else "",
                              cend.isoformat() if cend else "",
                              teachers,
                              cur if cur is not None else "",
                              fin if fin is not None else "",
                              passed, total, comp, unsubmitted, pct
                          ])
                  except Exception as e:
                      print(f"[warn] failed student {uid}: {e}", file=sys.stderr)
                      continue

          print("[ok] wrote Canvas_Enrollment_Courses.csv")
          PY

      - name: Prepare CSV for upsert (strip Enrollment Key)
        run: |
          python - <<'PY'
          import csv
          src = "Canvas_Enrollment_Courses.csv"
          dst = "Canvas_Enrollment_Courses_for_upsert.csv"
          with open(src, newline="", encoding="utf-8") as f, open(dst, "w", newline="", encoding="utf-8") as g:
              r = csv.DictReader(f)
              fields = [h for h in r.fieldnames if h != "Enrollment Key"]
              w = csv.DictWriter(g, fieldnames=fields)
              w.writeheader()
              for row in r:
                  row.pop("Enrollment Key", None)
                  w.writerow(row)
          print("[ok] wrote", dst)
          PY

      - name: Upload CSVs (artifact)
        uses: actions/upload-artifact@v4
        with:
          name: canvas-csvs-${{ matrix.key }}
          path: "*.csv"

      # ---------- ROBUST UPSERT (no performUpsert) ----------
      - name: Upsert Courses → Airtable (${{ matrix.table }})
        env:
          AIRTABLE_PAT:        ${{ secrets.AIRTABLE_PAT }}
          AIRTABLE_BASE_ID:    ${{ secrets.AIRTABLE_BASE_ID }}
          AIRTABLE_TABLE_NAME: ${{ matrix.table }}
        run: |
          echo "Upserting into Airtable table: $AIRTABLE_TABLE_NAME"
          python - <<'PY'
          import os, csv, requests, time, sys, json
          from urllib.parse import quote

          base  = os.environ["AIRTABLE_BASE_ID"]
          table = os.environ["AIRTABLE_TABLE_NAME"]
          token = os.environ["AIRTABLE_PAT"]
          api   = f"https://api.airtable.com/v0/{base}/{quote(table, safe='')}"
          hdr   = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}

          def batched(lst, n=10):
              for i in range(0, len(lst), n):
                  yield lst[i:i+n]

          # 1) Read CSV
          with open("Canvas_Enrollment_Courses_for_upsert.csv", newline="", encoding="utf-8") as f:
              rows = list(csv.DictReader(f))

          # 2) Fetch existing (id, key) to build map
          existing = {}
          offset = None
          while True:
              params = {"pageSize": 100}
              if offset: params["offset"] = offset
              r = requests.get(api, headers=hdr, params=params, timeout=60)
              if not r.ok:
                  print(f"[error] Read existing failed {r.status_code}: {r.text}")
                  r.raise_for_status()
              data = r.json()
              for rec in data.get("records", []):
                  key = (rec.get("fields",{}).get("Enrollment Course Key") or "").strip()
                  if key:
                      existing[key] = rec["id"]
              offset = data.get("offset")
              if not offset: break

          # 3) Split into updates vs creates
          updates, creates = [], []
          for row in rows:
              key = (row.get("Enrollment Course Key") or "").strip()
              if not key:
                  continue
              if key in existing:
                  updates.append({"id": existing[key], "fields": row})
              else:
                  creates.append({"fields": row})

          # 4) PATCH updates
          total_upd = total_new = 0
          for batch in batched(updates, 10):
              r = requests.patch(api, headers=hdr, json={"records": batch, "typecast": True}, timeout=60)
              if r.status_code in (429,500,502,503,504):
                  time.sleep(2); r = requests.patch(api, headers=hdr, json={"records": batch, "typecast": True}, timeout=60)
              if not r.ok:
                  print(f"[error] PATCH failed {r.status_code}: {r.text}")
                  r.raise_for_status()
              total_upd += len(batch)

          # 5) POST creates
          for batch in batched(creates, 10):
              r = requests.post(api, headers=hdr, json={"records": batch, "typecast": True}, timeout=60)
              if r.status_code in (429,500,502,503,504):
                  time.sleep(2); r = requests.post(api, headers=hdr, json={"records": batch, "typecast": True}, timeout=60)
              if not r.ok:
                  print(f"[error] POST failed {r.status_code}: {r.text}")
                  r.raise_for_status()
              total_new += len(batch)

          print(f"[ok] upsert complete. Updated={total_upd}, Created={total_new}")
          PY

      # NEW: Mark all current rows Active=true (so soft-delete can flip others to false)
      - name: Ensure current rows are Active=true
        env:
          AIRTABLE_PAT:        ${{ secrets.AIRTABLE_PAT }}
          AIRTABLE_BASE_ID:    ${{ secrets.AIRTABLE_BASE_ID }}
          AIRTABLE_TABLE_NAME: ${{ matrix.table }}
          COURSE_CSV:          Canvas_Enrollment_Courses_for_upsert.csv
        run: |
          python - <<'PY'
          import os, csv, requests, time, glob
          from urllib.parse import quote

          table = os.getenv("AIRTABLE_TABLE_NAME")
          base  = os.getenv("AIRTABLE_BASE_ID")
          token = os.getenv("AIRTABLE_PAT")
          csv_path = os.getenv("COURSE_CSV")

          if not table or not base or not token:
              print("[warn] Missing env for Ensure Active=true; skipping.")
              raise SystemExit(0)

          # Resolve CSV path(s)
          paths = []
          if csv_path and os.path.exists(csv_path):
              paths = [csv_path]
          else:
              paths = sorted(glob.glob("*Enrollment_Courses*.csv"))

          if not paths:
              print("[warn] No course CSV found; skipping Active=true step.")
              raise SystemExit(0)

          # Collect keys
          keys = set()
          for p in paths:
              with open(p, newline="", encoding="utf-8") as f:
                  for row in csv.DictReader(f):
                      k = (row.get("Enrollment Course Key") or "").strip()
                      if k:
                          keys.add(k)

          if not keys:
              print("[warn] No Enrollment Course Keys in CSV; nothing to activate.")
              raise SystemExit(0)

          api = f"https://api.airtable.com/v0/{base}/{quote(table, safe='')}"
          headers = {"Authorization": f"Bearer {token}"}

          # Build map key->record id
          id_by_key = {}
          offset = None
          while True:
              params = {"pageSize": 100}
              if offset: params["offset"] = offset
              r = requests.get(api, headers=headers, params=params, timeout=60); r.raise_for_status()
              j = r.json()
              for rec in j.get("records", []):
                  f = rec.get("fields", {})
                  k = (f.get("Enrollment Course Key") or "").strip()
                  if k:
                      id_by_key[k] = rec["id"]
              offset = j.get("offset")
              if not offset: break

          to_patch = [{"id": id_by_key[k], "fields": {"Active": True}} for k in keys if k in id_by_key]

          if not to_patch:
              print("[info] No matching records to mark Active=true.")
              raise SystemExit(0)

          def patch(batch):
              r = requests.patch(
                  api,
                  headers={**headers, "Content-Type": "application/json"},
                  json={"records": batch, "typecast": True},
                  timeout=60,
              )
              if r.status_code in (429,500,502,503,504):
                  time.sleep(2)
                  r = requests.patch(
                      api,
                      headers={**headers, "Content-Type": "application/json"},
                      json={"records": batch, "typecast": True},
                      timeout=60,
                  )
              r.raise_for_status()

          try:
              for i in range(0, len(to_patch), 10):
                  patch(to_patch[i:i+10])
              print(f"[ok] Marked {len(to_patch)} records Active=true in '{table}'.")
          except requests.HTTPError as e:
              if getattr(e.response, "status_code", None) == 422:
                  print(f"[warn] Could not write 'Active' (missing checkbox field?) for '{table}'.")
                  raise SystemExit(0)
              raise
          PY

      - name: Mark missing records as inactive (soft-delete)
        if: ${{ matrix.soft_delete != 'false' }}
        env:
          AIRTABLE_PAT:        ${{ secrets.AIRTABLE_PAT }}
          AIRTABLE_BASE_ID:    ${{ secrets.AIRTABLE_BASE_ID }}
          AIRTABLE_TABLE_NAME: ${{ matrix.table }}
        run: |
          python - <<'PY'
          import os, csv, requests, time, sys
          from urllib.parse import quote

          BASE  = os.environ["AIRTABLE_BASE_ID"]
          TABLE = os.environ["AIRTABLE_TABLE_NAME"]
          TOKEN = os.environ["AIRTABLE_PAT"]
          API   = f"https://api.airtable.com/v0/{BASE}/{quote(TABLE, safe='')}"
          H     = {"Authorization": f"Bearer {TOKEN}"}

          current_keys = set()
          with open("Canvas_Enrollment_Courses_for_upsert.csv", newline="", encoding="utf-8") as f:
            for r in csv.DictReader(f):
              k = (r.get("Enrollment Course Key") or "").strip()
              if k: current_keys.add(k)

          existing = []
          offset = None
          while True:
            params = {"pageSize": 100}
            if offset: params["offset"] = offset
            r = requests.get(API, headers=H, params=params, timeout=60)
            r.raise_for_status()
            j = r.json()
            for rec in j.get("records", []):
              fields = rec.get("fields", {})
              key = (fields.get("Enrollment Course Key") or "").strip()
              if key:
                existing.append((rec["id"], key))
            offset = j.get("offset")
            if not offset: break

          to_mark = [rid for rid, key in existing if key not in current_keys]
          print(f"[info] Existing: {len(existing)}, Current: {len(current_keys)}, Inactive to mark: {len(to_mark)}")

          try:
            for i in range(0, len(to_mark), 10):
              batch = [{"id": rid, "fields": {"Active": False}} for rid in to_mark[i:i+10]]
              r = requests.patch(API, headers={**H, "Content-Type":"application/json"},
                                 json={"records": batch, "typecast": True}, timeout=60)
              if r.status_code in (429,500,502,503,504):
                time.sleep(2)
                r = requests.patch(API, headers={**H, "Content-Type":"application/json"},
                                   json={"records": batch, "typecast": True}, timeout=60)
              r.raise_for_status()
            print("[ok] soft-delete complete")
          except requests.HTTPError as e:
            print(f"[warn] Soft-delete skipped for table '{TABLE}': {e}. "
                  f"Add an 'Active' checkbox field to enable soft-delete.")
            sys.exit(0)
          PY

  softdelete-b2c:
    needs: run-export
    runs-on: ubuntu-24.04
    steps:
      - name: Download B2C P1 CSV
        uses: actions/download-artifact@v4
        with:
          name: canvas-csvs-B2C_P1
          path: b2c-p1

      - name: Download B2C P2 CSV
        uses: actions/download-artifact@v4
        with:
          name: canvas-csvs-B2C_P2
          path: b2c-p2

      - name: Union soft-delete for B2C
        env:
          AIRTABLE_PAT:     ${{ secrets.AIRTABLE_PAT }}
          AIRTABLE_BASE_ID: ${{ secrets.AIRTABLE_BASE_ID }}
        run: |
          python - <<'PY'
          import os, csv, requests, time, sys
          from urllib.parse import quote

          TABLE = "B2C Courses Enrollment"
          BASE  = os.environ["AIRTABLE_BASE_ID"]
          TOKEN = os.environ["AIRTABLE_PAT"]
          API   = f"https://api.airtable.com/v0/{BASE}/{quote(TABLE, safe='')}"
          H     = {"Authorization": f"Bearer {TOKEN}"}

          # union of keys from chunks
          keyset = set()
          for folder in ("b2c-p1","b2c-p2"):
              if not os.path.isdir(folder): continue
              for name in os.listdir(folder):
                  if name.endswith(".csv"):
                      with open(os.path.join(folder, name), newline="", encoding="utf-8") as f:
                          for r in csv.DictReader(f):
                              k = (r.get("Enrollment Course Key") or "").strip()
                              if k: keyset.add(k)
          if not keyset:
              print("[warn] No keys from B2C chunks; skipping soft-delete."); sys.exit(0)

          # fetch existing
          existing = []
          offset = None
          while True:
              params = {"pageSize": 100}
              if offset: params["offset"] = offset
              r = requests.get(API, headers=H, params=params, timeout=60); r.raise_for_status()
              j = r.json()
              for rec in j.get("records", []):
                  k = (rec.get("fields",{}).get("Enrollment Course Key") or "").strip()
                  if k: existing.append((rec["id"], k))
              offset = j.get("offset")
              if not offset: break

          to_mark = [rid for rid, k in existing if k not in keyset]
          for i in range(0, len(to_mark), 10):
              batch = [{"id": rid, "fields": {"Active": False}} for rid in to_mark[i:i+10]]
              r = requests.patch(API, headers={**H, "Content-Type":"application/json"},
                                 json={"records": batch, "typecast": True}, timeout=60)
              if r.status_code in (429,500,502,503,504):
                  time.sleep(2)
                  r = requests.patch(API, headers={**H, "Content-Type":"application/json"},
                                     json={"records": batch, "typecast": True}, timeout=60)
              r.raise_for_status()
          print("[ok] B2C union soft-delete complete")
          PY
