name: Nightly Canvas Export (Students)

on:
  workflow_dispatch:
  schedule:
    - cron: "30 5 * * *"  # offset from Courses job

permissions:
  contents: read

jobs:
  run-export-students:
    runs-on: ubuntu-24.04
    strategy:
      fail-fast: false
      matrix:
        include:
          - key: B2C_P1
            table: "B2C Student Enrollments"
            soft_delete: "false"
          - key: B2C_P2
            table: "B2C Student Enrollments"
            soft_delete: "false"
          - key: SOUTHLANDS
            table: "Southlands Student Enrollments"
            soft_delete: "true"
          - key: PHOENIX
            table: "Phoenix Christian Student Enrollments"
            soft_delete: "true"
          - key: ONE_TWO_ONE_ONE
            table: "1211 Student Enrollments"
            soft_delete: "true"
          - key: NHCC
            table: "NHCC Student Enrollments"
            soft_delete: "true"
          - key: ROCKCHRISTIAN
            table: "Rock Christian Student Enrollments"
            soft_delete: "true"
          - key: GANEVAS
            table: "Ganevas Student Enrollments"
            soft_delete: "true"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests

      - name: Resolve Canvas IDs for cohort
        id: resolve_ids
        shell: bash
        env:
          INCLUDE_CANVAS_IDS_P1: ${{ secrets.INCLUDE_CANVAS_IDS_P1 }}
          INCLUDE_CANVAS_IDS_P2: ${{ secrets.INCLUDE_CANVAS_IDS_P2 }}
          IDS_SOUTHLANDS:        ${{ secrets.IDS_SOUTHLANDS }}
          IDS_PHOENIX:           ${{ secrets.IDS_PHOENIX }}
          IDS_1211:              ${{ secrets.IDS_1211 }}
          IDS_ROCKCHRISTIAN:     ${{ secrets.IDS_ROCKCHRISTIAN }}
          IDS_GANEVAS:           ${{ secrets.IDS_GANEVAS }}
          IDS_NHCC:              ${{ secrets.IDS_NHCC }}
        run: |
          case "${{ matrix.key }}" in
            B2C_P1)          echo "ids=$INCLUDE_CANVAS_IDS_P1" >> $GITHUB_OUTPUT ;;
            B2C_P2)          echo "ids=$INCLUDE_CANVAS_IDS_P2" >> $GITHUB_OUTPUT ;;
            SOUTHLANDS)      echo "ids=$IDS_SOUTHLANDS"        >> $GITHUB_OUTPUT ;;
            PHOENIX)         echo "ids=$IDS_PHOENIX"           >> $GITHUB_OUTPUT ;;
            ONE_TWO_ONE_ONE) echo "ids=$IDS_1211"              >> $GITHUB_OUTPUT ;;
            ROCKCHRISTIAN)   echo "ids=$IDS_ROCKCHRISTIAN"     >> $GITHUB_OUTPUT ;;
            GANEVAS)         echo "ids=$IDS_GANEVAS"           >> $GITHUB_OUTPUT ;;
            NHCC)            echo "ids=$IDS_NHCC"              >> $GITHUB_OUTPUT ;;
            *) echo "Unknown cohort key: ${{ matrix.key }}"; exit 1 ;;
          esac

      # ---------- Build per-student-year CSV ----------
      - name: Build per-student CSV for Airtable (${{ matrix.table }})
        env:
          API_URL: ${{ secrets.CANVAS_API_URL }}
          API_KEY: ${{ secrets.CANVAS_API_KEY }}
          IDS:     ${{ steps.resolve_ids.outputs.ids }}
        run: |
          python - <<'PY'
          import csv, os, time, sys
          from datetime import datetime
          import requests

          API_URL = os.environ["API_URL"].rstrip("/")
          API_KEY = os.environ["API_KEY"]
          IDS     = [s.strip() for s in (os.environ.get("IDS") or "").split(",") if s.strip()]
          HEADERS = {"Authorization": f"Bearer {API_KEY}"}
          PER_PAGE = 100
          START_MONTH = 8  # school year rollover (Aug)

          def iso(dt): return datetime.fromisoformat(dt.replace("Z","+00:00")) if dt else None
          def school_year(dt):
              if not dt: return ""
              y = dt.year
              return f"{y-1}-{y}" if dt.month < START_MONTH else f"{y}-{y+1}"

          def paged(url, params=None):
              params = dict(params or {})
              while True:
                  r = requests.get(url, headers=HEADERS, params=params, timeout=60)
                  if r.status_code in (429,500,502,503,504): time.sleep(1); continue
                  r.raise_for_status()
                  data = r.json()
                  if isinstance(data, list):
                      for x in data: yield x
                  else:
                      yield data
                  link = r.headers.get("Link","")
                  nxt = None
                  for part in link.split(","):
                      if 'rel="next"' in part:
                          nxt = part[part.find("<")+1: part.find(">")]
                          break
                  if not nxt: break
                  url, params = nxt, None

          if not IDS:
              print("[fatal] No Canvas IDs provided for this cohort.", file=sys.stderr)
              sys.exit(1)

          aggregates = {}  # key -> row

          for uid in IDS:
              try:
                  prof = requests.get(f"{API_URL}/users/{uid}/profile", headers=HEADERS, timeout=30)
                  p = prof.json() if prof.ok else {}
                  student_name = p.get("name") or f"User {uid}"
                  email = p.get("primary_email") or p.get("login_id") or ""

                  enrollments = list(paged(f"{API_URL}/users/{uid}/enrollments",
                                           {"type[]":"StudentEnrollment","include[]":"grades","per_page":PER_PAGE}))
                  by_year = {}
                  for enr in enrollments:
                      created = iso(enr.get("created_at"))
                      sy = school_year(created) or "unknown"
                      if sy == "unknown": continue
                      by_year.setdefault(sy, []).append(enr)

                  for sy, enrs in by_year.items():
                      key = f"{uid}-{sy}"
                      total_courses = 0
                      curr_scores, fin_scores = [], []
                      yr_total = yr_completed = yr_unsub = 0
                      initial_enrollment = None
                      observer_linked = "No"
                      bad = ("NPS","feedback","survey")

                      for enr in enrs:
                          created = iso(enr.get("created_at"))
                          if created and (initial_enrollment is None or created < initial_enrollment):
                              initial_enrollment = created
                          grades = enr.get("grades") or {}
                          if grades.get("current_score") is not None: curr_scores.append(grades["current_score"])
                          if grades.get("final_score")   is not None: fin_scores.append(grades["final_score"])
                          total_courses += 1
                          cid = enr.get("course_id")
                          if not cid: continue

                          total = comp = unsub = 0
                          for a in paged(f"{API_URL}/courses/{cid}/assignments"):
                              nm = (a.get("name") or "")
                              if not a.get("published"): continue
                              if nm == "End of Unit Feedback (Mandatory)": continue
                              if any(x.lower() in nm.lower() for x in bad): continue
                              total += 1
                              s = requests.get(f"{API_URL}/courses/{cid}/assignments/{a['id']}/submissions/{uid}",
                                               headers=HEADERS, timeout=30)
                              if s.ok:
                                  sub = s.json()
                                  if sub.get("excused"): comp += 1
                                  else:
                                      state = sub.get("workflow_state") or "unsubmitted"
                                      graded = sub.get("grade") not in (None,"-","")
                                      if state in ("graded","submitted") and graded: comp += 1
                                      else: unsub += 1
                              else:
                                  unsub += 1
                          yr_total += total; yr_completed += comp; yr_unsub += unsub

                          # observer link
                          if observer_linked != "Yes":
                              try:
                                  for obs in paged(f"{API_URL}/courses/{cid}/enrollments",
                                                   {"type[]":"ObserverEnrollment","include[]":"user","per_page":PER_PAGE}):
                                      if int(obs.get("associated_user_id") or 0) == int(uid):
                                          observer_linked = "Yes"; break
                              except Exception: pass

                      def avg(nums):
                          try:
                              vals = [float(x) for x in nums if x is not None]
                              return round(sum(vals)/len(vals), 2) if vals else ""
                          except Exception:
                              return ""

                      aggregates[key] = {
                          "Enrollment Key": key,
                          "Student Canvas ID": uid,
                          "Student Name": student_name,
                          "School Year": sy,
                          "Program": "",
                          "Initial Enrollment Date": initial_enrollment.isoformat() if initial_enrollment else "",
                          "Year Start Date": "",
                          "Year Completion Date": "",
                          "Overall Current Score": avg(curr_scores),
                          "Overall Final Score": avg(fin_scores),
                          "Notes": "",
                          "Total Courses": total_courses,
                          "Year Total Assignments": yr_total,
                          "Year Completed Assignments": yr_completed,
                          "Year Unsubmitted Assignments": yr_unsub,
                          "Student First Name": "",
                          "Student Last Name": "",
                          "Student ID nese ka (per SCS)": "",
                          "Student Date of Birth": "",
                          "Email": email,
                          "Parent First Name": "",
                          "Parent Last Name": "",
                          "Parent Email": "",
                          "Parent Phone Number": "",
                          "Parent Date of Birth": "",
                          "Address": "",
                          "Observer Account linked with student": observer_linked,
                      }
              except Exception as e:
                  print(f"[warn] failed student {uid}: {e}", file=sys.stderr)
                  continue

          out = "Canvas_Enrollments.csv"
          headers = [
            "Enrollment Key","Student Canvas ID","Student Name","School Year","Program",
            "Initial Enrollment Date","Year Start Date","Year Completion Date",
            "Overall Current Score","Overall Final Score","Notes","Total Courses",
            "Year Total Assignments","Year Completed Assignments","Year Unsubmitted Assignments",
            "Student First Name","Student Last Name","Student ID nese ka (per SCS)",
            "Student Date of Birth","Email","Parent First Name","Parent Last Name",
            "Parent Email","Parent Phone Number","Parent Date of Birth","Address",
            "Observer Account linked with student"
          ]
          with open(out, "w", newline="", encoding="utf-8") as f:
              w = csv.DictWriter(f, fieldnames=headers)
              w.writeheader()
              for row in aggregates.values():
                  w.writerow({h: row.get(h, "") for h in headers})

          print(f"[ok] wrote {out} with {len(aggregates)} rows.")
          PY

      - name: Upload CSVs (artifact)
        uses: actions/upload-artifact@v4
        with:
          name: student-csvs-${{ matrix.key }}
          path: "*.csv"

      # Ensure current run rows are Active=true so new/updated rows are active
      - name: Ensure current STUDENT rows are Active=true
        if: ${{ matrix.soft_delete != 'false' }}
        env:
          AIRTABLE_PAT:        ${{ secrets.AIRTABLE_PAT }}
          AIRTABLE_BASE_ID:    ${{ secrets.AIRTABLE_BASE_ID }}
          AIRTABLE_TABLE_NAME: ${{ matrix.table }}
        run: |
          python - <<'PY'
          import os, csv, requests, sys
          from urllib.parse import quote

          BASE  = os.environ["AIRTABLE_BASE_ID"]
          TABLE = os.environ["AIRTABLE_TABLE_NAME"]
          TOKEN = os.environ["AIRTABLE_PAT"]
          API   = f"https://api.airtable.com/v0/{BASE}/{quote(TABLE, safe='')}"
          H     = {"Authorization": f"Bearer {TOKEN}", "Content-Type":"application/json"}

          def s(v): return str(v).strip() if v is not None else ""

          keys = []
          try:
            with open("Canvas_Enrollments.csv", newline="", encoding="utf-8") as f:
              for r in csv.DictReader(f):
                k = s(r.get("Enrollment Key"))
                if k: keys.append(k)
          except FileNotFoundError:
            print("[warn] No Enrollment Keys in CSV; skipping Active=true step.")
            sys.exit(0)

          if not keys:
            print("[warn] No Enrollment Keys in CSV; skipping Active=true step.")
            sys.exit(0)

          existing = {}
          offset = None
          while True:
            params = {"pageSize": 100}
            if offset: params["offset"] = offset
            r = requests.get(API, headers={"Authorization": f"Bearer {TOKEN}"}, params=params, timeout=60)
            r.raise_for_status()
            j = r.json()
            for rec in j.get("records", []):
              key = s((rec.get("fields", {}) or {}).get("Enrollment Key"))
              if key: existing[key] = rec["id"]
            offset = j.get("offset")
            if not offset: break

          to_true = [existing[k] for k in keys if k in existing]
          for i in range(0, len(to_true), 10):
            batch = [{"id": rid, "fields": {"Active": True}} for rid in to_true[i:i+10]]
            r = requests.patch(API, headers=H, json={"records": batch, "typecast": True}, timeout=60)
            if not r.ok and r.status_code not in (404, 422):
              r.raise_for_status()
          print(f"[ok] ensured {len(to_true)} rows Active=true.")
          PY

      # ---------- Upsert (auto-trim to Airtable's existing fields; always include key) ----------
      - name: Upsert Students â†’ Airtable (${{ matrix.table }})
        env:
          AIRTABLE_PAT:        ${{ secrets.AIRTABLE_PAT }}
          AIRTABLE_BASE_ID:    ${{ secrets.AIRTABLE_BASE_ID }}
          AIRTABLE_TABLE_NAME: ${{ matrix.table }}
        run: |
          echo "Upserting into Airtable table: $AIRTABLE_TABLE_NAME"
          python - <<'PY'
          import os, csv, requests, time, sys
          from urllib.parse import quote

          base  = os.environ["AIRTABLE_BASE_ID"]
          table = os.environ["AIRTABLE_TABLE_NAME"]
          token = os.environ["AIRTABLE_PAT"]
          api   = f"https://api.airtable.com/v0/{base}/{quote(table, safe='')}"
          hdr   = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}

          KEY_FIELD = "Enrollment Key"

          def batched(lst, n=10):
              for i in range(0, len(lst), n):
                  yield lst[i:i+n]

          # read csv
          with open("Canvas_Enrollments.csv", newline="", encoding="utf-8") as f:
              r = csv.DictReader(f)
              csv_headers = r.fieldnames or []
              rows = list(r)

          if KEY_FIELD not in csv_headers:
              print(f"[fatal] CSV missing required key field: {KEY_FIELD}", file=sys.stderr)
              sys.exit(1)

          # discover fields present in Airtable (infer from first pages)
          existing_fields = set()
          offset = None
          while True:
              params = {"pageSize": 100}
              if offset: params["offset"] = offset
              rr = requests.get(api, headers=hdr, params=params, timeout=60)
              if not rr.ok:
                  print(f"[error] Field discovery failed {rr.status_code}: {rr.text[:300]}")
                  rr.raise_for_status()
              data = rr.json()
              for rec in data.get("records", []):
                  for k in (rec.get("fields") or {}).keys():
                      existing_fields.add(k)
              offset = data.get("offset")
              if not offset:
                  break
          if not existing_fields:
              print("[warn] Table appears empty; proceeding with only key + any matching fields in future pages.")
          existing_fields.add(KEY_FIELD)

          # log skipped cols
          skipped = [c for c in csv_headers if c not in existing_fields]
          if skipped:
              print("[info] Skipping CSV columns not present in Airtable:")
              for s in skipped: print("  -", s)

          # map existing key -> record id
          existing_map = {}
          offset = None
          while True:
              params = {"pageSize": 100}
              if offset: params["offset"] = offset
              rr = requests.get(api, headers=hdr, params=params, timeout=60)
              if not rr.ok:
                  print(f"[error] Read existing failed {rr.status_code}: {rr.text[:300]}")
                  rr.raise_for_status()
              data = rr.json()
              for rec in data.get("records", []):
                  key = str((rec.get("fields",{}).get(KEY_FIELD) or "")).strip()
                  if key: existing_map[key] = rec["id"]
              offset = data.get("offset")
              if not offset: break

          # split updates/creates, trimmed to existing fields
          updates, creates = [], []
          allowed = existing_fields
          for row in rows:
              key = str((row.get(KEY_FIELD) or "")).strip()
              if not key: continue
              trimmed = {k: v for k, v in row.items() if k in allowed}
              if key in existing_map:
                  updates.append({"id": existing_map[key], "fields": trimmed})
              else:
                  creates.append({"fields": trimmed})

          # PATCH updates
          total_upd = total_new = 0
          for batch in batched(updates, 10):
              rr = requests.patch(api, headers=hdr, json={"records": batch, "typecast": True}, timeout=60)
              if rr.status_code in (429,500,502,503,504):
                  time.sleep(2); rr = requests.patch(api, headers=hdr, json={"records": batch, "typecast": True}, timeout=60)
              if not rr.ok:
                  print(f"[error] PATCH failed {rr.status_code}: {rr.text[:500]}"); rr.raise_for_status()
              total_upd += len(batch)

          # POST creates
          for batch in batched(creates, 10):
              rr = requests.post(api, headers=hdr, json={"records": batch, "typecast": True}, timeout=60)
              if rr.status_code in (429,500,502,503,504):
                  time.sleep(2); rr = requests.post(api, headers=hdr, json={"records": batch, "typecast": True}, timeout=60)
              if not rr.ok:
                  print(f"[error] POST failed {rr.status_code}: {rr.text[:500]}"); rr.raise_for_status()
              total_new += len(batch)

          print(f"[ok] upsert complete. Updated={total_upd}, Created={total_new}")
          PY

      # ---------- Soft-delete (mark Inactive) ----------
      # Marks records Inactive if EITHER:
      #  - Enrollment Key is missing this run, OR
      #  - Student Canvas ID is not present this run (student left).
      - name: Mark missing STUDENT records as inactive (soft-delete)
        if: ${{ matrix.soft_delete != 'false' }}
        env:
          AIRTABLE_PAT:        ${{ secrets.AIRTABLE_PAT }}
          AIRTABLE_BASE_ID:    ${{ secrets.AIRTABLE_BASE_ID }}
          AIRTABLE_TABLE_NAME: ${{ matrix.table }}
        run: |
          python - <<'PY'
          import os, csv, requests, time, sys
          from urllib.parse import quote

          BASE  = os.environ["AIRTABLE_BASE_ID"]
          TABLE = os.environ["AIRTABLE_TABLE_NAME"]
          TOKEN = os.environ["AIRTABLE_PAT"]
          API   = f"https://api.airtable.com/v0/{BASE}/{quote(TABLE, safe='')}"
          H     = {"Authorization": f"Bearer {TOKEN}", "Content-Type":"application/json"}

          def s(v): return str(v).strip() if v is not None else ""

          # Read current run keys & student IDs
          current_keys = set()
          current_sids = set()
          with open("Canvas_Enrollments.csv", newline="", encoding="utf-8") as f:
              for r in csv.DictReader(f):
                  k = s(r.get("Enrollment Key"))
                  sid = s(r.get("Student Canvas ID"))
                  if k: current_keys.add(k)
                  if sid: current_sids.add(sid)

          # Fetch all existing records (id, key, student id)
          existing = []
          offset = None
          while True:
              params = {"pageSize": 100}
              if offset: params["offset"] = offset
              resp = requests.get(API, headers={"Authorization": f"Bearer {TOKEN}"}, params=params, timeout=60)
              resp.raise_for_status()
              j = resp.json()
              for rec in j.get("records", []):
                  flds = rec.get("fields", {})
                  key = s(flds.get("Enrollment Key"))
                  sid = s(flds.get("Student Canvas ID"))
                  existing.append((rec["id"], key, sid))
              offset = j.get("offset")
              if not offset: break

          # Inactivate if missing by Enrollment Key OR by Student Canvas ID
          to_mark = []
          for rid, key, sid in existing:
              if (key and key not in current_keys) or (sid and sid not in current_sids):
                  to_mark.append(rid)

          print(f"[info] Existing: {len(existing)}, Current keys: {len(current_keys)}, Current sids: {len(current_sids)}, Inactive to mark: {len(to_mark)}")

          try:
            for i in range(0, len(to_mark), 10):
              batch = [{"id": rid, "fields": {"Active": False}} for rid in to_mark[i:i+10]]
              r = requests.patch(API, headers=H, json={"records": batch, "typecast": True}, timeout=60)
              if r.status_code in (429,500,502,503,504):
                time.sleep(2)
                r = requests.patch(API, headers=H, json={"records": batch, "typecast": True}, timeout=60)
              r.raise_for_status()
            print("[ok] soft-delete complete")
          except requests.HTTPError as e:
            print(f"[warn] Soft-delete skipped for table '{TABLE}': {e}. Ensure an 'Active' checkbox exists.")
            sys.exit(0)
          PY

  # ---------- Union soft-delete for B2C Students across P1/P2 ----------
  softdelete-b2c-students:
    needs: run-export-students
    runs-on: ubuntu-24.04
    steps:
      - name: Download B2C P1 CSV
        uses: actions/download-artifact@v4
        with:
          name: student-csvs-B2C_P1
          path: b2c-p1
      - name: Download B2C P2 CSV
        uses: actions/download-artifact@v4
        with:
          name: student-csvs-B2C_P2
          path: b2c-p2

      - name: Union soft-delete for B2C Students
        env:
          AIRTABLE_PAT:     ${{ secrets.AIRTABLE_PAT }}
          AIRTABLE_BASE_ID: ${{ secrets.AIRTABLE_BASE_ID }}
        run: |
          python - <<'PY'
          import os, csv, requests, time, sys
          from urllib.parse import quote

          def s(v): return str(v).strip() if v is not None else ""

          TABLE = "B2C Student Enrollments"
          BASE  = os.environ["AIRTABLE_BASE_ID"]
          TOKEN = os.environ["AIRTABLE_PAT"]
          API   = f"https://api.airtable.com/v0/{BASE}/{quote(TABLE, safe='')}"
          H     = {"Authorization": f"Bearer {TOKEN}", "Content-Type":"application/json"}

          # Union by Enrollment Key and Student ID across both chunks
          keys, sids = set(), set()
          for folder in ("b2c-p1","b2c-p2"):
              if not os.path.isdir(folder): continue
              for name in os.listdir(folder):
                  if name.endswith(".csv"):
                      with open(os.path.join(folder, name), newline="", encoding="utf-8") as f:
                          for r in csv.DictReader(f):
                              k = s(r.get("Enrollment Key"))
                              sid = s(r.get("Student Canvas ID"))
                              if k: keys.add(k)
                              if sid: sids.add(sid)

          if not keys and not sids:
              print("[warn] No keys or student IDs from B2C chunks; skipping soft-delete."); sys.exit(0)

          # Fetch existing Airtable records
          existing = []
          offset = None
          while True:
              params = {"pageSize": 100}
              if offset: params["offset"] = offset
              r = requests.get(API, headers={"Authorization": f"Bearer {TOKEN}"}, params=params, timeout=60); r.raise_for_status()
              j = r.json()
              for rec in j.get("records", []):
                  f = rec.get("fields", {})
                  k = s(f.get("Enrollment Key"))
                  sid = s(f.get("Student Canvas ID"))
                  existing.append((rec["id"], k, sid))
              offset = j.get("offset")
              if not offset: break

          to_mark = [rid for rid, k, sid in existing if (k and k not in keys) or (sid and sid not in sids)]
          for i in range(0, len(to_mark), 10):
              batch = [{"id": rid, "fields": {"Active": False}} for rid in to_mark[i:i+10]]
              r = requests.patch(API, headers=H, json={"records": batch, "typecast": True}, timeout=60)
              if r.status_code in (429,500,502,503,504):
                  time.sleep(2)
                  r = requests.patch(API, headers=H, json={"records": batch, "typecast": True}, timeout=60)
              r.raise_for_status()
          print("[ok] B2C student union soft-delete complete")
          PY
